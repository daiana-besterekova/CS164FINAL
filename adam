def norm(x):
    return sqrt(x[0]^2 + x[1]^2)

def grad(f, x):
    h = 1e-5
    gradient = []

    for i, xi in enumerate(x):
        def f_i(x):
            x[i] += h
            fx_plus_h = f(x)
            x[i] -= h
            return fx_plus_h

        partial_derivative = (f_i(x) - f(x)) / h
        gradient.append(partial_derivative)

    return(gradient[0], gradient[1])

def function(x):

    #ROSENBROCK: (x[0]-1)**2 + 100*(x[1]-x[0]**2)**2
    #TEST2: sin(x[0])*cos(x[1])
    #TEST3: -(x[0]*sin(x[1])*(3 - x[0] - x[0]*sin(x[1])) + (x[0]*cos(x[1])*x[0]*sin(x[1]))/2)
    return  -(x[0]*sin(x[1])*(3 - x[0] - x[0]*sin(x[1])) + (x[0]*cos(x[1])*x[0]*sin(x[1]))/2)


def adam( x, y, num_iterations, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):
    m_x = 0
    m_y = 0
    v_x = 0
    v_y = 0
    
    all_points_x = []
    all_points_y = []
  
    for t in range(num_iterations):
        grad_x, grad_y  = vector((grad(function, [x, y])))

    # Update moving averages of gradients
        m_x = beta1 * m_x + (1 - beta1) * grad_x
        m_y = beta1 * m_y + (1 - beta1) * grad_y

        # Update moving averages of squared gradients
        v_x = beta2 * v_x + (1 - beta2) * grad_x**2
        v_y = beta2 * v_y + (1 - beta2) * grad_y**2

        # Correct bias in moving averages
        m_x_hat = m_x / (1 - beta1**(t+1))
        m_y_hat = m_y / (1 - beta1**(t+1))
        v_x_hat = v_x / (1 - beta2**(t+1))
        v_y_hat = v_y / (1 - beta2**(t+1))

        # Update variables x and y
        x -= learning_rate * m_x_hat / (sqrt(v_x_hat) + epsilon)
        y -= learning_rate * m_y_hat / (sqrt(v_y_hat) + epsilon)

        all_points_x.append(x)
        all_points_y.append(y)
    return x, y, all_points_x, all_points_y


# Initialize variables x and y
x = 1
y = 1.2

# Run Adam optimization algorithm
x, y, allx, ally = adam(x, y, num_iterations=50, learning_rate=0.001)

print(x, y)


x,y = var('x,y')

import matplotlib.pyplot as plt
import numpy as np



# generate a grid of points
x = np.linspace(0.84, 1.05, 100)
y = np.linspace(1, 1.3, 100)
X, Y = np.meshgrid(x, y)

# evaluate the function on the grid
Z = function([X, Y])

# plot the contour of the function
plt.contour(X, Y, Z, levels=200)
plt.scatter(allx[-1], ally[-1], color = 'red')
plt.plot(allx, ally, color = 'black', linewidth = 0.5)
plt.scatter(allx, ally, color = 'red', s=0.5)

# show the plot
plt.show()

